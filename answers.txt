1 (d) [TODO]

2 (f) "Explain your tokenizer function and discuss its performance."
There are three stages in my tokeniser function.

First, mentions of the names of constituencies represented by each party are replaced by a token (or, more accurately, what will become a token) indicating that the constituency is represented by that party. For example, "Kingston and Surbiton" is replaced by "LiberalDemocratSEAT". To determine what constituencies each party represents, for convenience I used the Hansard dataset itself, extracting those constituencies only associated with a single party, but in principle this information could be obtained from an external source instead. (This is done after cleaning the Hansard dataset, so Labour and Labour (Co-op) are counted as the same party and only the 4 most common parties (which turn out to be Conservative, Labour, Liberal Democrat, and Scottish National Party) have their constituency names substituted in this way).

There are two main reasons why a constituency name would appear in an MP's speech. The obvious reason is that they are talking about something in their local area, in which case knowing which party represents that area immediately tells us the speaker's party. However, the other is to refer to other MPs, as they are not allowed to refer to each other by name in the House of Commons and will instead often refer to a particular MP by the constituency they represent. So naming a constituency could also indicate that the speaker is referring to another MP, potentially of a different party. Either way, in a party-political context I conjecture that the most relevant part of the 'meaning' of constituency names is what party they are represented by, hence these substitutions.

Next, the text is split into words by taking any sequence of non-alphanumeric characters other than apostrophes and hyphens to be a separator, and normalised into lower case. 

In the final stage, two things happen simultaneously: stopwords, taken from the NLTK list of English stopwords, are discarded, and '_NEG' is appended to each token that occurs between any of a pre-defined set of 'negative words' and punctuation (which for this purpose is taken to be any non-space separator between words). (The reason that these happen simultaneously is that many of the negative words are stopwords, so they can't be discared before this stage, but they also cannot be discarded afterwards as some occurences will then have had a _NEG appended).

The 'negative words' used here are slightly more broad than is standard, including not only words like 'not' and 'isn't' but also some words expressing negative valence such as 'bad' or 'incompetent'. The idea here is to capture whether certain concepts are being talked about in a positive or negative context. For example, it would be expected that the governing party (which was the Conservatives in the period that the dataset covers) would talk positively about the state of the economy, health service, etc., whilest the opposition parties would be expected to mention these in more negative contexts.

Performance:

The best performance using this tokeniser with a tf-idf vectoriser limited to 3000 features was found when considering unigrams, bigrams, and trigrams, and using an SVM as the classifier. On almost every metric in the classification model this performed better on the test set than both the RandomForests and SVMs from 2(c) and 2(d) that used the standard scikit-learn tokeniser, in particular achieving 84% accuracy compared to the 80% achieved by the best of the 4 classifiers using the standard tokeniser (i.e. the SVM that considers unigrams, bigrams, and trigrams). The exceptions are recall for 'Conservative' and precision for 'Scottish National Party', which were both higher for both of the RandomForest classifiers using the standard tokeniser, however these classifiers are much less accurate over all, achieving only 74% accuracy even when considering unigrams, bigrams, and trigrams.

In terms of computational efficiency, with my custom tokeniser and considering unigrams, bigrams, and trigrams it took 47.0 seconds to vectorise both the training and testing texts (8084 speeches in total), compared to under 17.7 seconds with the default tokeniser when considering unigrams, bigrams, and trigrams, and 5.0 seconds with the default tokeniser when considering only unigrams.
